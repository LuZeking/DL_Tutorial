{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"I Target： Here we learn how to make dataset\n",
    "\n",
    "II Definition: In Keras, Dataset is not actual a images-set, it's a defined class, if you want to design your own\n",
    " Dataset, you should inherit from torch.utils.data.Dataset. Actually, you can get images and labels by __getitem__,\n",
    " where they store in torch.utils.data.Dataset.\n",
    "    \n",
    "III Instances:\n",
    "    There are 3 ways to load and preprocess image dataset \n",
    "    1.0 build tf.data.Dataset ,  by random x,y sample\n",
    "    2.0 use high-level Keras preprocessing utilities(e.g. tf.keras.utils.image_dataset_from_directory)\n",
    "        and layers (e.g. tf.keras.layers.Rescaling)\n",
    "    3.0 down load tensorflow datasets\n",
    "\n",
    "\n",
    "IV Compare 2 then Generalize\n",
    "\n",
    "V Test in New instance \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset_by_random_sample():\n",
    "    \"\"\"1.0&1.1 build tf.data.Dataset, by  random x&y \n",
    "        quickest way to build tf.data.Dataset is from tf.data.Dataset.from_tensor_slices\n",
    "    \"\"\"\n",
    "    x = np.linspace(-1, 1, 10)\n",
    "    y = x**2\n",
    "\n",
    "    ds_without_labels = tf.data.Dataset.from_tensor_slices(x)  # ,y\n",
    "    print(f\"ds_without_labels: {ds_without_labels}\")\n",
    "\n",
    "    dataset  = tf.data.Dataset.from_tensor_slices((x,y))  \n",
    "    print(f\"dataset with labels:{dataset}\")\n",
    "\n",
    "    # or you can zip x and label y\n",
    "    y_ds = tf.data.Dataset.from_tensor_slices(y)\n",
    "    images_label_ds = tf.data.Dataset.zip((ds_without_labels,y_ds))\n",
    "    print(f\"zipped_images_label_ds:{images_label_ds}\")\n",
    "\n",
    "    # print(ds_without_labels[0])  # 'TensorSliceDataset' object does not support indexing\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds_without_labels: <TensorSliceDataset shapes: (), types: tf.float64>\n",
      "dataset with labels:<TensorSliceDataset shapes: ((), ()), types: (tf.float64, tf.float64)>\n",
      "zipped_images_label_ds:<ZipDataset shapes: ((), ()), types: (tf.float64, tf.float64)>\n"
     ]
    }
   ],
   "source": [
    "build_dataset_by_random_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_inner_dataset():\n",
    "    \"\"\"1.2  \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader_exsample():\n",
    "    \"\"\"2.0 Dataloader \n",
    "        use tf.data to shuffle, batch, repeat...\n",
    "    \"\"\"\n",
    "\n",
    "    x = np.linspace(-1, 1, 10)\n",
    "    y = x**2\n",
    "\n",
    "    dataset  = tf.data.Dataset.from_tensor_slices((x,y))  \n",
    "    # print(f\"dataset with labels:{dataset}\")\n",
    "\n",
    "    BATCH_SIZE = 5\n",
    "    dataloader = dataset.shuffle(buffer_size=len(x))\n",
    "    dataloader = dataloader.repeat()\n",
    "    dataloader = dataloader.batch(BATCH_SIZE)\n",
    "\n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    dataloader = dataloader.prefetch(buffer_size=AUTOTUNE)\n",
    "    \"\"\"\"↑↑↑↑  Creates a `Dataset` that prefetches elements from this dataset.\n",
    "\n",
    "                Most dataset input pipelines should end with a call to `prefetch`. This\n",
    "                allows later elements to be prepared while the current element is being\n",
    "                processed. This often improves latency and throughput, at the cost of\n",
    "                using additional memory to store prefetched elements.\n",
    "    \"\"\"\n",
    "    print(f\"dataloader:{dataloader}\")\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def dataloader_MNIST():\n",
    "    \"\"\"2.0 Dataloader \n",
    "        use tf.data to shuffle, batch, repeat...\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (x_train.reshape(60000, 784).astype(\"float32\") / 255, y_train)\n",
    "    )\n",
    "    \n",
    "    # print(f\"dataset with labels:{dataset}\")\n",
    "\n",
    "    BATCH_SIZE = 1\n",
    "    dataloader = dataset.shuffle(buffer_size=1024)\n",
    "    dataloader = dataloader.repeat()\n",
    "    dataloader = dataloader.batch(BATCH_SIZE)\n",
    "\n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    dataloader = dataloader.prefetch(buffer_size=AUTOTUNE)\n",
    "    \"\"\"\"↑↑↑↑  Creates a `Dataset` that prefetches elements from this dataset.\n",
    "\n",
    "                Most dataset input pipelines should end with a call to `prefetch`. This\n",
    "                allows later elements to be prepared while the current element is being\n",
    "                processed. This often improves latency and throughput, at the cost of\n",
    "                using additional memory to store prefetched elements.\n",
    "    \"\"\"\n",
    "    print(f\"dataloader:{dataloader}\")\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataloader:<PrefetchDataset shapes: ((None,), (None,)), types: (tf.float64, tf.float64)>\n",
      "dataloader:<PrefetchDataset shapes: ((None, 784), (None,)), types: (tf.float32, tf.uint8)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None, 784), (None,)), types: (tf.float32, tf.uint8)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader_exsample()\n",
    "dataloader_MNIST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_MLP():\n",
    "    \"\"\"3.1 Stack layers Model \n",
    "       inhiert from keras.layers.Layer!\n",
    "    \"\"\"\n",
    "    from tensorflow import keras\n",
    "    \n",
    "    class Linear(keras.layers.Layer):\n",
    "        \"\"\"y = w.x + b\"\"\"\n",
    "\n",
    "        def __init__(self, units=32, input_dim=32):\n",
    "            super(Linear, self).__init__()\n",
    "            w_init = tf.random_normal_initializer()\n",
    "            self.w = tf.Variable(\n",
    "                initial_value=w_init(shape=(input_dim, units), dtype=\"float32\"),\n",
    "                trainable=True,\n",
    "            )\n",
    "            b_init = tf.zeros_initializer()\n",
    "            self.b = tf.Variable(\n",
    "                initial_value=b_init(shape=(units,), dtype=\"float32\"), trainable=True\n",
    "            )\n",
    "\n",
    "        def call(self, inputs):\n",
    "            return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "    class MLP(keras.layers.Layer):\n",
    "        def __init__(self):\n",
    "            super(MLP, self).__init__()\n",
    "            self.linear_1 = Linear(32)\n",
    "            self.linear_2 = Linear(32)\n",
    "            self.linear_3 = Linear(10)\n",
    "\n",
    "        def call(self, inputs):\n",
    "            x = self.linear_1(inputs)\n",
    "            x = tf.nn.relu(x)\n",
    "            x = self.linear_2(x)\n",
    "            x = tf.nn.relu(x)\n",
    "            return self.linear_3(x)\n",
    "    \n",
    "    mlp = MLP()\n",
    "    print(f\"MLP_model:{mlp}\")\n",
    "    # y = mlp(tf.ones(shape=(3, 64)))\n",
    "    # assert len(mlp.weights) == 6\n",
    "    return mlp\n",
    "\n",
    "def model_Sequential_exsample():\n",
    "    \"\"\"3.2 Sequential model\n",
    "       use keras.Sequential to build model\n",
    "    \"\"\"\n",
    "    from tensorflow import keras\n",
    "    classes = 10\n",
    "\n",
    "    Sequential_model = keras.Sequential(\n",
    "    [keras.layers.Dense(64, activation=tf.nn.relu), keras.layers.Dense(classes),]\n",
    "    ) \n",
    "\n",
    "    print(f\"Sequential_model:{Sequential_model}\")\n",
    "\n",
    "    return Sequential_model\n",
    "\n",
    "def  functional_model():\n",
    "    \"\"\"3.3 build model functionally\"\"\"\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_model:<__main__.call_MLP.<locals>.MLP object at 0x2b4079986b70>\n",
      "Sequential_model:<tensorflow.python.keras.engine.sequential.Sequential object at 0x2b4079986ac8>\n"
     ]
    }
   ],
   "source": [
    "call_MLP()\n",
    "model_Sequential_exsample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_model:<__main__.call_MLP.<locals>.MLP object at 0x2b407af40160>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/938 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Matrix size-incompatible: In[0]: [64,784], In[1]: [32,32] [Op:MatMul]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-f52902bce99e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0;31m#! forward, edfault gradien zero?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# more with zero_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# loss.backward() in torch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# in torch training paras setting in optim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/rds/user/hpczeji1/hpc-work/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-62fc042843e6>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/rds/user/hpczeji1/hpc-work/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-62fc042843e6>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/rds/user/hpczeji1/hpc-work/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/rds/user/hpczeji1/hpc-work/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   3253\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3254\u001b[0m       return gen_math_ops.mat_mul(\n\u001b[0;32m-> 3255\u001b[0;31m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   3256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/rds/user/hpczeji1/hpc-work/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   5622\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5623\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5624\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5625\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5626\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/rds/user/hpczeji1/hpc-work/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6842\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6843\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6844\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/rds/user/hpczeji1/hpc-work/anaconda3/envs/py36/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Matrix size-incompatible: In[0]: [64,784], In[1]: [32,32] [Op:MatMul]"
     ]
    }
   ],
   "source": [
    "\"\"\"4.0 Optimzer \"\"\"\n",
    "# 1&2 Dataset and Dataloader\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.reshape(60000, 784).astype(\"float32\") / 255, y_train)\n",
    ")\n",
    "dataloader = dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# 3 model\n",
    "mlp = call_MLP()\n",
    "\n",
    "# 4 optimzer\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)  # Whether `y_pred` is expected to be a logits tensor\n",
    "\n",
    "#TODO ADD hypercolumn to LCN and loss\n",
    "\n",
    "\n",
    "\"\"\"5.0 Train \"\"\"\n",
    "from tqdm import tqdm\n",
    "epoch_num = 2\n",
    "for epoch in range(epoch_num):\n",
    "    with tqdm(dataloader) as train_bar:\n",
    "        for step, (x,y) in enumerate(train_bar):\n",
    "            \n",
    "            with tf.GradientTape() as tape:  # in torch, without this GradientTape\n",
    "                \n",
    "                #! forward, edfault gradien zero?\n",
    "                loss = loss_fn(y, mlp(x)) # more with zero_grad\n",
    "                loss += sum(mlp.losses)# loss.backward() in torch\n",
    "                gradient = tape.gradient(loss, mlp.trainable_weights) # in torch training paras setting in optim\n",
    "            \n",
    "            optimizer.apply_gradients(zip(gradient, mlp.trainable_weights))# in torch optimizer.step()\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        print(\"Epoch\",epoch, \"Step:\", step, \"Loss:\", float(loss))\n",
    "\n",
    "\n",
    "# time = str(datetime.now()).split(\" \")[0].replace(\"-\", \"_\")\n",
    "# torch.save(model.state_dict(), \"model_{}.pth\".format(time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16c11565bb4e67660010f3b8ac54bb06ff920d6e5d1ce8d761516dd991d6b185"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('py36': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
