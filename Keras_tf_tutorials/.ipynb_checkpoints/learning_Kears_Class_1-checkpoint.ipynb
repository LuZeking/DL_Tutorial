{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- 1 张量、变量、运算\n",
    "- 2 通过继承Layer类来建立自定义的神经网络层\n",
    "- 3 手写循环来更好地自定义对神经网络训练过程。\n",
    "- 4  使用add_loss()方法来自定义神经网络层的损失函数\n",
    "- 5 在手写循环训练过程中对评估标准（metrics）进行追踪。\n",
    "- 6 使用tf.function来编译并加速神经网络的执行过程。\n",
    "- 7 神经网络层的运行：训练模式 vs 推断（inference）模式。\n",
    "Keras的函数式API。\n",
    "\n",
    "\n",
    "\n",
    "## 1 张量、变量、运算、梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "type & shape : (tf.int32, TensorShape([2, 2]))\n",
      "\n",
      "tf.Tensor(\n",
      "[[1.]\n",
      " [1.]], shape=(2, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.]\n",
      " [0.]], shape=(2, 1), dtype=float32)\n",
      "\n",
      "a&b：<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
      "array([[-0.48557258,  0.1765048 ],\n",
      "       [-0.11463156, -0.12760173]], dtype=float32)>&<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
      "array([[-2.9563994 ,  0.46518046],\n",
      "       [-0.67977923,  2.0084083 ]], dtype=float32)>\n",
      "\n",
      "the new a is <tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
      "array([[ 1.8382372 ,  0.21799925],\n",
      "       [-2.3676453 ,  0.9096812 ]], dtype=float32)>\n",
      "\n",
      " c is [[-1.0976415  1.2751701]\n",
      " [-4.0345764  4.051833 ]]\n",
      "\n",
      " gradien c to a : [[-0.515997    0.41984028]\n",
      " [-0.23810868 -0.8262035 ]]\n",
      "\n",
      " gradient d2c to d2a is [[1.1605387  0.46659344]\n",
      " [2.4311388  0.13421854]]\n"
     ]
    }
   ],
   "source": [
    "# 1 张量、变量、运算、梯度\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)\n",
    "\n",
    "# 1.1 创建一个2维固定张量constant, numpy() can convert tensor to array\n",
    "x = tf.constant([[1,2],[5,3]])\n",
    "x\n",
    "print(f\"type & shape : {x.dtype,x.shape}\\n\")\n",
    "x.numpy()\n",
    "\n",
    "# create tensors use ones,zero,random\n",
    "print(tf.ones(shape=(2,1)))\n",
    "print(tf.zeros(shape=(2,1)))\n",
    "\n",
    "x = tf.random.normal(shape=(2,2),mean =0.0,stddev=1.0)\n",
    "# x = tf.random.uniform(shape=(2,2),minval=0,maxval=10,dtype=\"int32\")\n",
    "x\n",
    "\n",
    "\n",
    "# 1.2 张量 Varibale is a special tensor, which is adaptable, using  tf.Varibale(x) to convert\n",
    "inital_value = tf.random.normal(shape=(2,2))\n",
    "a =  tf.Variable(inital_value)\n",
    "b= tf.Variable(x)\n",
    "print(f\"\\na&b：{a}&{b}\")\n",
    "\n",
    "# using .assgin/assgin_add/assign_sub()来指定新的值，加减值\n",
    "new_value = tf.random.normal(shape=(2,2))\n",
    "a.assign(new_value)\n",
    "print(f\"\\nthe new a is {a}\")\n",
    "# check if the computation is correct？\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        assert a[i,j]==new_value[i,j]\n",
    "\n",
    "subbed_value = tf.random.normal(shape=(2,2))\n",
    "a.assign_sub(subbed_value)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        assert a[i,j] == new_value[i,j]-subbed_value[i,j]\n",
    "\n",
    "# 1.3 computation\n",
    "c = a+b\n",
    "print(f\"\\n c is {c}\")\n",
    "d = tf.square(c)\n",
    "d\n",
    "e =tf.exp(d)\n",
    "d\n",
    "\n",
    "# 1.4 gradient 梯度计算\n",
    "del a,b,c\n",
    "a = tf.random.normal(shape=(2,2))\n",
    "b = tf.random.normal(shape=(2,2))\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(a)\n",
    "    c= tf.sqrt(tf.square(a)+tf.square(b))\n",
    "    # calculate gradien c to a\n",
    "    dc_da = tape.gradient(c,a)\n",
    "    print(f\"\\n gradien c to a : {dc_da}\")\n",
    "\n",
    "a = tf.Variable(a)  # when no tape.watch,set a to Variable type is significant\n",
    "with tf.GradientTape() as outer_tape:\n",
    "    with tf.GradientTape() as tape:\n",
    "        c = tf.sqrt(tf.square(a)+tf.square(b))\n",
    "        dc_da = tape.gradient(c,a)\n",
    "    d2c_d2a = outer_tape.gradient(dc_da,a)\n",
    "    print(f\"\\n gradient d2c to d2a is {d2c_d2a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2  通过继承Layer类来建立自定义的神经网络层\n",
    "- TensorFlow是可微分编程的基础框架，主要用于处理张量、变量和梯度\n",
    "- Keras是深度学习的用户接口，主要用于处理神经网络层、模型、优化器、损失函数、评估标准等等。\n",
    "    - Keras作为TensorFlow的上层API，使得TensorFlow更加简单用，以提高工程师们的生产力。\n",
    "    - Layer类是Keras中最基础的一种抽象，表示神经网络的层。 它对权重和一些（在call()方法中定义的）计算表达式进行了封装。\n",
    "\n",
    "### 2.1 layer的继承与调用\n",
    "- 初始化后，可以把Layer的对象当成一个Python函数来用。\n",
    "- 在__init__函数中简历的权重变量会自动被追踪，它们都被放在了weights这个属性里面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear layer' weights: [<tf.Variable 'Variable:0' shape=(2, 4) dtype=float32, numpy=\n",
      "array([[ 0.04057386, -0.04837519, -0.0169137 , -0.03372589],\n",
      "       [-0.04266707,  0.01906256,  0.10583218, -0.09750512]],\n",
      "      dtype=float32)>, <tf.Variable 'Variable:0' shape=(4,) dtype=float32, numpy=array([-0.00361824, -0.0066523 ,  0.02731263,  0.01057839], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    \"\"\"y = w.x + b\"\"\"\n",
    "\n",
    "    def __init__(self,units=32,input_dim=32):\n",
    "        super(Linear, self).__init__()  # make Linear callable\n",
    "        w_init = tf.random_normal_initializer()  # 均匀分布uniform, 高斯分布normal\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=w_init(shape =(input_dim,units),dtype = \"float32\"),\n",
    "                                 trainable = True,\n",
    "        )\n",
    "        b_init = tf.random_normal_initializer()\n",
    "        self.b = tf.Variable(\n",
    "            initial_value = b_init(shape=(units,),dtype=\"float32\"),\n",
    "                                 trainable=True,\n",
    "        )\n",
    "\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "# init layer\n",
    "linear_layer =Linear(units=4,input_dim=2)\n",
    "\n",
    "# input some data\n",
    "y = linear_layer(tf.ones((2,2)))\n",
    "assert y.shape == (2,4)\n",
    "\n",
    "assert linear_layer.weights == [linear_layer.w,linear_layer.b]\n",
    "print(f\"linear layer' weights: {linear_layer.weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.2 给layer添加权重\n",
    "- 一种直接在init里定义（如上），第二种即通过define build函数就可以（如下），方便地添加权重\n",
    "- ♥♥♥当网络复杂后，权重如何添加？---定义灭一层layer中的网络权重即可，复杂网络也是由一层层layer组成的\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    \"\"\"y = wx+b\"\"\"\n",
    "\n",
    "    def __init__(self,units=784):  # units是设定输出的神经元数量，input_shape是自动获取的输入维度\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1],self.units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return tf.matmul(inputs,self.w)+self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.3 layer的梯度 (以单层FCN训练Mnist为例)\n",
    "- 通过GradientTape调用一个layer来自动获取梯度\n",
    "- 通过梯度进而可以更新一个layer的权重\n",
    "- 更新可以通过优化器自动进行，也可以自己写代码更新\n",
    "- 有必要的话，你也可以在更新前修改梯度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step：0 Loss: 2.3591556549072266\n",
      "Step：100 Loss: 2.3170151710510254\n",
      "Step：200 Loss: 2.1400370597839355\n",
      "Step：300 Loss: 2.0045042037963867\n",
      "Step：400 Loss: 1.9647746086120605\n",
      "Step：500 Loss: 1.9063069820404053\n",
      "Step：600 Loss: 1.7791277170181274\n",
      "Step：700 Loss: 1.7752561569213867\n",
      "Step：800 Loss: 1.6594003438949585\n",
      "Step：900 Loss: 1.610144853591919\n"
     ]
    }
   ],
   "source": [
    "# 1 数据准备 Minst  60000*32*32*1-> 60000*784\n",
    "(x_train, y_train),_ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.reshape(60000,784).astype(\"float32\")/255,y_train)\n",
    ")\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(64) # 数据集划分batch,\n",
    "\n",
    "# 2 初始化上述线性layer，有10个神经元\n",
    "linear_layer = Linear(10)\n",
    "# 初始化一个逻辑损失函式，接受整数值的标签\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# 初始化一个优化器\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "# 循环数据集中所有的batch\n",
    "for step,(x,y) in enumerate(dataset):\n",
    "\n",
    "    # 创建一个GradientTape\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        # feedword\n",
    "        logits = linear_layer(x)\n",
    "\n",
    "        # 获取对于当前batch的损失\n",
    "        loss = loss_fn(y,logits)\n",
    "\n",
    "    # 求损失函式关于权重的梯度\n",
    "    gradients = tape.gradient(loss, linear_layer.trainable_weights)\n",
    "\n",
    "    # update weights\n",
    "    optimizer.apply_gradients(zip(gradients,linear_layer.trainable_weights))\n",
    "\n",
    "    # output\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step：{step} Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.4 可训练与不可训练的权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2.]\n",
      "[4. 4.]\n"
     ]
    }
   ],
   "source": [
    "class ComputeSum(keras.layers.Layer):\n",
    "    \"\"\"Compute the sum of inputs\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "        super(ComputeSum,self).__init__()\n",
    "        # build an untrainable weight and weight is the return\n",
    "        self.total = tf.Variable(initial_value=tf.zeros((input_dim,)),trainable=False)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.total.assign_add(tf.reduce_sum(inputs,axis = 0 ))\n",
    "        return self.total\n",
    "\n",
    "my_sum = ComputeSum(2)\n",
    "x = tf.ones((2, 2))\n",
    "\n",
    "y = my_sum(x)\n",
    "print(y.numpy())\n",
    "\n",
    "y = my_sum(x)    \n",
    "print(y.numpy())\n",
    "\n",
    "assert my_sum.weights == [my_sum.total]\n",
    "assert my_sum.non_trainable_weights == [my_sum.total]\n",
    "assert my_sum.trainable_weights == []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.5 layer的嵌套使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# build a MLP, which includes multi linear layers\n",
    "\n",
    "class MLP(keras.layers.Layer):\n",
    "    \"\"\"Simple stack of Linear Layers\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.linear_2 = Linear(32)\n",
    "        self.linear_3 = Linear(10)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x =tf.nn.relu(x)\n",
    "        return self.linear_3(x)\n",
    "\n",
    "mlp = MLP()\n",
    "\n",
    "# 下面这句首次调用了MLP，也就是添加了所有权重\n",
    "y =mlp(tf.ones(shape = (3,64)))\n",
    "assert len(mlp.weights)  == 6  # 3 layers, each layer has w&b # print(f\"mlp weights : {mlp.weights}\")\n",
    "\n",
    "# above eequal to\n",
    "mlp = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(32,activation=tf.nn.relu),\n",
    "        keras.layers.Dense(32,activation=tf.nn.relu),\n",
    "        keras.layers.Dense(10)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.6 给layer添加loss并追踪layer产生的损失函数\n",
    "    Layer可是在正向传播过程中使用add_loss()方法来添加损失函数。 一种很好的把正则化加入损失函数的方法。 子Layer所添加的损失函数也会自动被母Layer所追踪。下面的例子是把activity正则化加入了损失函数。\n",
    "- 在layer 的 call() 里面用self.add_loss\n",
    "- ♥如何添加自定义loss？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivityRegularization(keras.layers.Layer):\n",
    "    \"\"\"一个把activity稀疏正则化加入损失函数的Layer。本身应该没有新增结构\"\"\"\n",
    "    \n",
    "    def __init__(self, rate = 1e-2):\n",
    "        super(ActivityRegularization, self).__init__()\n",
    "        self.rate = rate\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        #  使用`add_loss`来添加正则化损失函数，用输入的张量计算\n",
    "        self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个模型只要使用了这个Layer，就会自动把这个正则化的损失函数加入到整体损失函数中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=0.16962673>]\n"
     ]
    }
   ],
   "source": [
    "class SparseMLP(keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SparseMLP,self).__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.regulariztion = ActivityRegularization(1e-2)\n",
    "        self.linear_2 = Linear(10)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.regulariztion(x)\n",
    "        return self.linear_2(x)\n",
    "    \n",
    "mlp = SparseMLP()\n",
    "y = mlp(tf.ones((10,10)))\n",
    "print(mlp.losses)  # 一个模型只要使用了这个Layer，就会自动把这个正则化的损失函数加入到整体损失函数中。        \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=0.17540042>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=0.17540042>]\n"
     ]
    }
   ],
   "source": [
    "mlp = SparseMLP()\n",
    "\n",
    "mlp(tf.ones((10,10)))\n",
    "assert len(mlp.losses) == 1\n",
    "print(mlp.losses)\n",
    "\n",
    "mlp(tf.ones((10,10)))\n",
    "assert len(mlp.losses) == 1\n",
    "print(mlp.losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在每次正向传播开始，最外层的Layer会先把损失函数值清空，以防止把上一次正向传播产生的损失函数加进来。layer.loss永远只包含最近一次完成的正向传播所产生的的损失函数。你在自己写训练循环的时候，在用这些损失函数来计算梯度之前，你需要先把他们加起来求和。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.7 追踪训练的评估标准（Metrics）\n",
    "- Keras有很多的内置评估标准，比如tf.keras.metrics.AUC和tf.keras.metrics.PrecisionAtRecall。 \n",
    "- 你也可以写一个你自己的评估标准，几行代码就能搞定。\n",
    "- 在一个手写的训练循环里面使用评估标准，你要做如下几件事：\n",
    "    - 初始化评估标准，比如tf.keras.metrics.AUC()。\n",
    "    - 在每个batch中调用函数metric.update_state(targets, predictions)来更新评估标准的统计值。 =\n",
    "    - 用metric.results()来查询评估标准当前的值。\n",
    "    - 在每个epoch结尾或者开始时用metric.reset_states()来重置评估标准的值。\n",
    "- 下面是一个简单的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Step: 0\n",
      "Total running accuracy so far: 0.094\n",
      "Epoch: 0 Step: 200\n",
      "Total running accuracy so far: 0.922\n",
      "Epoch: 0 Step: 400\n",
      "Total running accuracy so far: 0.969\n",
      "Epoch: 0 Step: 600\n",
      "Total running accuracy so far: 0.938\n",
      "Epoch: 0 Step: 800\n",
      "Total running accuracy so far: 0.922\n",
      "Epoch: 1 Step: 0\n",
      "Total running accuracy so far: 0.906\n",
      "Epoch: 1 Step: 200\n",
      "Total running accuracy so far: 0.953\n",
      "Epoch: 1 Step: 400\n",
      "Total running accuracy so far: 0.953\n",
      "Epoch: 1 Step: 600\n",
      "Total running accuracy so far: 0.938\n",
      "Epoch: 1 Step: 800\n",
      "Total running accuracy so far: 0.922\n"
     ]
    }
   ],
   "source": [
    "# 1 init\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "# prepare layer,loss function, optimzer\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(32, activation = \"relu\"),\n",
    "        keras.layers.Dense(32, activation = \"relu\"),\n",
    "        keras.layers.Dense(10),\n",
    "    ]\n",
    ")\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
    "optimzer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "for epoch in range(2):\n",
    "    # traverse over batches\n",
    "    for step, (x,y) in enumerate(dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x)\n",
    "            loss_value = loss_fn(y,logits)\n",
    "        \n",
    "        # 2 update accuracy(metrics)\n",
    "        accuracy.update_state(y,logits)\n",
    "        \n",
    "        gradients = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimzer.apply_gradients(zip(gradients,model.trainable_weights))\n",
    "        \n",
    "        if step % 200 == 0:\n",
    "            print(\"Epoch:\", epoch, \"Step:\", step)\n",
    "            # print metrics result\n",
    "            print(\"Total running accuracy so far: %.3f\" % accuracy.result())\n",
    "\n",
    "        # reser metrics in the end of every epoch\n",
    "        accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### self.add_loss() & self.add_metrics()\n",
    "和self.add_loss()类似，你也可以使用self.add_metric()来添加任何一个变量进去作为评估标准。\n",
    "可以用layer.reset_metrics()来重置某一层或整个模型的评估标准的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3 函数编译、训练与推断\n",
    "### 3.1 编译加速\n",
    "- 执行时我们使用的是Eager模式，调试程序比较方便。\n",
    "- 与之对应的静态图模式则在执行时更快。\n",
    "- 你只要把一个函数用`@tf.function`这个装饰器装饰一下，那个函数就会以静态图模式执行了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Loss: 2.1089882850646973\n",
      "Step: 100 Loss: 0.6745302677154541\n",
      "Step: 200 Loss: 0.20523759722709656\n",
      "Step: 300 Loss: 0.19360283017158508\n",
      "Step: 400 Loss: 0.16870474815368652\n",
      "Step: 500 Loss: 0.2925932705402374\n",
      "Step: 600 Loss: 0.12460373342037201\n",
      "Step: 700 Loss: 0.1298626959323883\n",
      "Step: 800 Loss: 0.10817199945449829\n",
      "Step: 900 Loss: 0.19529452919960022\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# prepare model \n",
    "model = keras.Sequential(\n",
    "        [\n",
    "            keras.layers.Dense(32,activation = \"relu\"),\n",
    "            keras.layers.Dense(32,activation = \"relu\"),\n",
    "            keras.layers.Dense(10),\n",
    "        ]\n",
    ")\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# build a function to execute one batch\n",
    "@tf. function # use this to boost speed\n",
    "def train_on_batch(x,y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(y, logits)\n",
    "        gradients = tape.gradient(loss, model.trainable_weights)\n",
    "    optimzer.apply_gradients(zip(gradients,model.trainable_weights))\n",
    "    return loss\n",
    "\n",
    "# prepare data\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.reshape(60000,784).astype(\"float32\")/255, y_train)\n",
    ")\n",
    "dataset = dataset.shuffle(buffer_size = 1024).batch(64)\n",
    "\n",
    "for step, (x,y) in enumerate(dataset):\n",
    "    loss = train_on_batch(x,y)\n",
    "    if step % 100 ==0:\n",
    "        loss =train_on_batch(x,y)\n",
    "        if step % 100 ==0:\n",
    "            print(\"Step:\",step, \"Loss:\", float(loss))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 训练和推断\n",
    "- 某些神经网络的层在训练和推断的时候采用的是不同的计算公式，也就是说，他们在训练和推断的时候做的事情是不一样的。\n",
    "- 对于这种情况，我们有一种标准操作来帮你拿到现在究竟是在训练还是在推断这个信息。 \n",
    "- 那就是在call函数里面有一个布尔型参数叫training。\n",
    "- 在你自定义层的call函数里面使用这个参数，可以帮你正确地实现好该层在训练和推断时候的不同表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(keras.layers.Layer):\n",
    "    def __init__(self,rate):\n",
    "        super(Dropout,self).__init__()\n",
    "        self.rate = rate\n",
    "        \n",
    "    def call(self, inputs, training =None):\n",
    "        if training:\n",
    "            return tf.nn.dropout(inputs, rate = self.rate)\n",
    "        return inputs\n",
    "\n",
    "class MLPwithDropout(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(MLPwithDropout, self).__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.linear_3 = Linear(10)\n",
    "        \n",
    "    def call(self, inputs,training =None):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.dropout(x, training = training)\n",
    "        return self.linear_3(x)\n",
    "\n",
    "mlp = MLPwithDropout()\n",
    "y_train = mlp(tf.ones((2, 2)), training=True)\n",
    "y_test = mlp(tf.ones((2, 2)), training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 用函数式API建立模型\n",
    "\n",
    "- 要建立模型的话，你不一定非要用面向对象的方法（即把模型定义为完整的一个类）。\n",
    "- 我们之前所看到的Layer是可以用函数式的方法来组合到一起的，我们称之为函数式API。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用`Input`对象来指定输入的尺寸和类型，就像我们给变量定义一个类型一样\n",
    "# `shape`是用来描述单个样本的尺寸的，不包含batch大小这个维度\n",
    "# 函数式API旨在定义单个样本在模型里面的变化过程\n",
    "# 模型会自动把这些对单个样本的操作打包成对batch的操作，然后按batch来处理数据\n",
    "inputs = tf.keras.Input(shape = (16,), dtype = \"float32\")\n",
    "\n",
    "# 我们把这些Layer当函数调用，并输入这些标记着变量类型和尺寸的对象\n",
    "# 我们会收到一个标记着新的尺寸和类型的对象\n",
    "x = Linear(32)(inputs)\n",
    "x = Dropout(0.5)(x)\n",
    "outputs = Linear(10)(x)\n",
    "\n",
    "# 一个函数式的`Model`对象可以用它的输入和输出来进行定义\n",
    "# 一个`Model`对象本身也是一个`Layer`\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "# 一个函数式的模型在我们用数据调用它之前就已经有了权重\n",
    "# 因为我们建立模型时就定义了它的输入尺寸，它也就可以推理出所有权重的尺寸了\n",
    "assert len(model.weights)==4\n",
    "\n",
    "y = model(tf.ones((2,16)))\n",
    "assert y.shape == (2,10)\n",
    "\n",
    "y = model(tf.ones((2,16)),training = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 函数式API比继承的方式更简洁，并且还有一些别的优势，即函数式编程相比于面向对象编程的一些优势。 \n",
    "- 但是，函数式API只能用来定义一些有向无环图（DAG），对于循环神经网络来讲，我们就必须使用继承的方式来实现。\n",
    "\n",
    "[这里](https://keras.io/guides/functional_api/)有更详尽的函数式API教程。\n",
    "\n",
    "- 在你建立模型的过程中，可能通常要使用函数式和继承这两种建模方法的结合。\n",
    "\n",
    "- Model类的另一个比较好的特性是有fit()和evaluate()这些内置的函数。\n",
    "你可以继承Model类，就像我们继承Layer类一样，然后重载这些函数来实现你自己的训练和评估循环。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 完整例子 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例子1：变分自编码器\n",
    "我们先来总结一下我们已经学到的内容：\n",
    "- 一个`Layer`对象可以封装一些状态值（在`__init__`或者`build`里面定义的权重等等）和一些计算式（在`call`里面定义的）。\n",
    "- Layer可以递归式地组合在一起来构建更大的包含更多计算的Layer。\n",
    "- 你可以自定义的训练循环，你只需要开启`GradientTape`，在其内部调用你的模型来获取梯度，并传递给优化器即可。\n",
    "- 你可以用`@tf.function`装饰器来给你的训练循环加速。\n",
    "- 可以用`self.add_loss()`来给Layer添加损失函数，一般是用来添加正则化的。\n",
    "\n",
    "我们接下来就把这些内容一起放进一个完整的例子里。我们来写一变分自编码器（Variational Autoencoder, VAE），并且使用MNIST数据集来进行训练。\n",
    "\n",
    "VAE会继承Layer类，并嵌套调用其他Layer。还会使用KL散度作为正则化的损失函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们需要一个Encoder编码器类，它把一个MNIST手写数字图片来对应到一个在潜在空间（latent space）里面的三元组(z_mean, z_log_var, z)，过程中使用了一个Sampling采样层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"使用`(z_mean, z_log_var)`来采样获得对一个数字的编码z\"\"\"\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch,dim))\n",
    "        return z_mean +tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, latent_dim=32, intermediate_dim=64, **kwargs):\n",
    "        super(Encoder,self).__init__(**kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation = tf.nn.relu)\n",
    "        self.dense_mean = layers.Dense(latent_dim)\n",
    "        self.dense_log_var = layers.Dense(latent_dim)\n",
    "        self.sampling = Sampling()\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        z_mean = self.dense_mean(x)\n",
    "        z_log_var = self.dense_log_var(x)\n",
    "        z = self.sampling((z_mean, z_log_var))\n",
    "        return z_mean, z_log_var, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(layers.Layer):\n",
    "    \"\"\"把编码成的向量z转化回原来的数字图片\"\"\"\n",
    "\n",
    "    def __init__(self, original_dim, intermediate_dim=64, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation=tf.nn.relu)\n",
    "        self.dense_output = layers.Dense(original_dim, activation=tf.nn.sigmoid)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        return self.dense_output(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们的VariationalAutoEncoder变分自编码器类会把编码器和解码器串起来，然后用add_loss()来加入KL散度正则化的损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoEncoder(layers.Layer):\n",
    "    \"\"\"把编码器和解码器串起来形成一个完成的模型用于训练\"\"\"\n",
    "    \n",
    "    def __init__(self, original_dim, intermediate_dim=64, latent_dim=32, **kwargs):\n",
    "        super(VariationalAutoEncoder, self).__init__(**kwargs)\n",
    "        self.original_dim = original_dim\n",
    "        self.encoder = Encoder(latent_dim=latent_dim, intermediate_dim=intermediate_dim)\n",
    "        self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        # 把KL散度正则化加入损失函数\n",
    "        kl_loss = -0.5 * tf.reduce_mean(\n",
    "            z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1\n",
    "        )\n",
    "        self.add_loss(kl_loss)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Loss: 0.35167396068573\n",
      "Step: 100 Loss: 0.12733835992541645\n",
      "Step: 200 Loss: 0.10050324391369796\n",
      "Step: 300 Loss: 0.09021177672567558\n",
      "Step: 400 Loss: 0.08513512817255577\n",
      "Step: 500 Loss: 0.08186289471036898\n",
      "Step: 600 Loss: 0.07945196085087274\n",
      "Step: 700 Loss: 0.07803974776248448\n",
      "Step: 800 Loss: 0.07677401215786792\n",
      "Step: 900 Loss: 0.07583355329327128\n",
      "Step: 1000 Loss: 0.07487215196246748\n"
     ]
    }
   ],
   "source": [
    "# 初始化模型\n",
    "vae = VariationalAutoEncoder(original_dim=784, intermediate_dim=64, latent_dim=32)\n",
    "\n",
    "# 损失函数和优化器\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# 准备数据\n",
    "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    ")\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(32)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def training_step(x):\n",
    "    with tf.GradientTape() as tape:\n",
    "        reconstructed = vae(x)  # 计算出重建的输入图片\n",
    "        # 计算损失函数值\n",
    "        loss = loss_fn(x, reconstructed)\n",
    "        loss += sum(vae.losses)  # 加上KL散度的损失函数\n",
    "    # 更新VAE的权重\n",
    "    grads = tape.gradient(loss, vae.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
    "    return loss\n",
    "\n",
    "\n",
    "losses = []  # 用于记录过程中产生的损失函数值\n",
    "for step, x in enumerate(dataset):\n",
    "    loss = training_step(x)\n",
    "    # 输出日志\n",
    "    losses.append(float(loss))\n",
    "    if step % 100 == 0:\n",
    "        print(\"Step:\", step, \"Loss:\", sum(losses) / len(losses))\n",
    "\n",
    "    # 1000步之后停止\n",
    "    # 把模型训练至收敛就当成留给你的练习题了\n",
    "    if step >= 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 函数式API快捷实现\n",
    "- 由此可见，用Keras来建立和训练这样的模型是很快很容易实现的。\n",
    "\n",
    "- 现在你可能觉得上面的代码还是有点啰嗦，这是因为我们每个细节都是亲自用代码实现的。这让我们有了最大的自由度，但同时也增加了我们的工作量。\n",
    "\n",
    "- 我们看看用函数式API来实现的话怎么样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dim = 784\n",
    "intermediate_dim = 64\n",
    "latent_dim = 32\n",
    "\n",
    "# 编码器\n",
    "\n",
    "# 解码器\n",
    "\n",
    "# VAE模型\n",
    "\n",
    "# 添加KL散度正则化损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外，Keras还有内置的训练和评估循环，是Model类的方法（fit()和evaluate()）。代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例子2：超网络\n",
    "- 超网络是一种特殊的深度神经网络，它的权重是用另一个神经网络生成出来的。（通常另一个神经网络要更小一些）。\n",
    "\n",
    "- 我们来实现一个非常简单的超网络。我们用一个两层的神经网络来输出另一个三层的神经网络的权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras是一个强有力的生产力工具，让你能轻松实现任何的科研想法。试想一下，你可能可以在一天之内尝试25个不同的想法（每20分钟一个）！\n",
    "\n",
    "Keras的宗旨之一就是从以最快的速度把想法变成结果，我们相信这是做出伟大科研成果的关键。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
