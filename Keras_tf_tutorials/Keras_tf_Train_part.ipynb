{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train: here we build a train pipline, e.g. CIFAR 10 \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR10 x_train shape:(50000, 32, 32, 3), y_train shape : (50000, 1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 1&2 Dataset and Dataloader\n",
    "\"\"\"\n",
    "(x_train, y_train), (x_test, y_test)  = tf.keras.datasets.cifar10.load_data()\n",
    "print(f\"CIFAR10 x_train shape:{x_train.shape}, y_train shape : {y_train.shape}\")\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.reshape(50000, 32, 32, 3).astype(\"float32\") / 255, y_train)  # not reshape ot 1024*3\n",
    ")\n",
    "BATCH_SIZE = 64\n",
    "dataloader = dataset.shuffle(buffer_size=1024).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no need to change LCN strcture, only need to add new loss\\n    steps 1 : lean how add loss\\n          2 : create own spatial loss: reshape weight to 2D sheet, the loss value = neighbour similarity equation\\n          3 : apply final loss = neighbour similarity + calssification\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"3 model\n",
    "    1 build own layer\n",
    "    2 build Conv layer\n",
    "    \"\"\"\n",
    "class Hypercolum_layer(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "      super(Hypercolum_layer,self).__init__\n",
    "    #   self.filters = 64\n",
    "    #   self.kernel_size = (3,3)\n",
    "    #   self.activation = tf.nn.relu\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # LCN_1 =  keras.layers.Conv2D(64, kernel_size=(3,3), activation=tf.nn.relu)\n",
    "        # x = LCN_1(inputs)\n",
    " \n",
    "        return inputs\n",
    "\n",
    "#! 定义理解错了，定义model和定义layer形式不同，在layer中直接增加loss即可\n",
    "class Hypercolum_layer(keras.layers.Layer):\n",
    "    \"\"\"一个把activity稀疏正则化加入损失函数的Layer。\"\"\"\n",
    "\n",
    "    def __init__(self, rate=1e-2):\n",
    "        super(Hypercolum_layer, self).__init__()\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # 使用`add_loss`来添加正则化损失函数，用输入的张量计算\n",
    "        self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
    "        LCN_1 =  keras.layers.Conv2D(64, kernel_size=(3,3), activation=tf.nn.relu)  #! 不是在call中建模的 加入__init__再调用或许可以\n",
    "        x = LCN_1(inputs)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CNN_3L(keras.layers.Layer):\n",
    "    \"\"\"3 layers CNN.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN_3L, self).__init__() \n",
    "        self.conv_1 = keras.layers.Conv2D(64,  kernel_size=(3,3), activation=tf.nn.relu) # \n",
    "        self.flatten = keras.layers.Flatten(data_format='channels_last')\n",
    "        self.linear = keras.layers.Dense(10)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv_1(inputs)\n",
    "        # x = tf.nn.relu(x)\n",
    "        x = self.flatten(x)\n",
    "        # x = tf.nn.relu(x)\n",
    "        self.add_loss(tf.reduce_sum(inputs))  #TODO 修改loss即可完成Spatial Loss的定义\n",
    "        return self.linear(x)\n",
    "\n",
    "model= CNN_3L()\n",
    "\n",
    "# 2 functional stack\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "inputs = tf.keras.Input(shape=(32,32,3))  # ,batch_size=64\n",
    "# d = tf.keras.layers.Dense(10)\n",
    "# LCN_1 = keras.layers.Conv2D(64,  kernel_size=(3,3), activation=tf.nn.relu)\n",
    "LCN_1 =  keras.layers.LocallyConnected2D(64, kernel_size=(3,3), activation=tf.nn.relu)\n",
    "x = LCN_1(inputs)\n",
    "Flatten_1 = keras.layers.Flatten(data_format='channels_last')\n",
    "x = Flatten_1(x)\n",
    "outputs = tf.keras.layers.Dense(10)(x)\n",
    "\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "\n",
    "# Weight regularization.\n",
    "print(LCN_1.kernel.shape)  # LCN weight shape (900, 27, 64)(position_num,kernel3*3*3,channels), while CNN weight shape is  (3, 3, 3, 64)(image_channel, kernelsize1&2,weight channels)\n",
    "#!current reshape way may not be right, better the weight be square\n",
    "# 1 reshape to 2d sheet \n",
    "position_num,kernel_size_with_c,weight_channels = LCN_1.kernel.shape\n",
    "sheet_2D_kernel_LCN_1 = tf.reshape(LCN_1.kernel, (30*3*8, 30*9*8)) #  ((position_num**0.5)*kernel_size_with_c*weight_channels, (position_num**0.5)*kernel_size_with_c*weight_channels)\n",
    "# sheet_2D_kernel_LCN_1 = LCN_1.kernel.reshape((position_num**0.5)*kernel_size_with_c*weight_channels, (position_num**0.5)*kernel_size_with_c*weight_channels)  # (30*27*64, 30*27*64)\n",
    "print(f\"after reshape to 2D sheet:{sheet_2D_kernel_LCN_1.shape}\")\n",
    "top_left_LCN_1 = sheet_2D_kernel_LCN_1[:-1,:-1]\n",
    "bottom_right_LCN_1 = sheet_2D_kernel_LCN_1[1:, 1:]\n",
    "cosine_similarity_LCN_1 = np.dot(top_left_LCN_1, tf.transpose(bottom_right_LCN_1))/(np.linalg.norm(top_left_LCN_1)*np.linalg.norm(bottom_right_LCN_1))\n",
    "loss_hypercolum = tf.reduce_mean((1-cosine_similarity_LCN_1)/2)\n",
    "print(f\"loss_hypercolum:{loss_hypercolum}\")\n",
    "# 2 sheet implement equation\n",
    "alpha = 0.1\n",
    "model.add_loss(lambda: alpha*loss_hypercolum)  # tf.reduce_mean(sheet_2D_kernel_LCN_1)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 3 Sequential\n",
    "# model = keras.Sequential(\n",
    "#     [\n",
    "#         # keras.layers.Conv2D(64,  kernel_size=(3,3), activation=tf.nn.relu), # input_shape=(3, 32, 32),\n",
    "#         # keras.layers.Conv2D(64, kernel_size=(3,3), activation=tf.nn.relu),\n",
    "#         # keras.layers.Conv2D(64, kernel_size=(3,3), activation=tf.nn.relu),\n",
    "#         keras.layers.LocallyConnected2D(64, kernel_size=(3,3), activation=tf.nn.relu),\n",
    "#         # keras.layers.LocallyConnected2D(64, kernel_size=(3,3), activation=tf.nn.relu),\n",
    "#         # keras.layers.LocallyConnected2D(64, kernel_size=(3,3), activation=tf.nn.relu),\n",
    "#         keras.layers.Flatten(data_format='channels_last'),\n",
    "#         keras.layers.Dense(10),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "#fixdeTODO add spatial loss\n",
    "\"\"\"no need to change LCN strcture, only need to add new loss\n",
    "    steps 1 : lean how add loss\n",
    "          2 : create own spatial loss: reshape weight to 2D sheet, the loss value = neighbour similarity equation\n",
    "          3 : apply final loss = neighbour similarity + calssification\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (50000, 1)\n",
      "1563/1563 [==============================] - 5s 4ms/step - loss: 46527.8984 - accuracy: 0.2511\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cnn_3l_16 (CNN_3L)           (None, 10)                577802    \n",
      "_________________________________________________________________\n",
      "softmax_13 (Softmax)         (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 577,802\n",
      "Trainable params: 577,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"2.0 train and test in keras way \"\"\"\n",
    "model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])\n",
    "\n",
    "\n",
    "model.compile(optimizer=\"SGD\", loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits= True),metrics=['accuracy'])\n",
    "\n",
    "\n",
    "reshaped_x_train = x_train.reshape(50000, 32, 32, 3).astype(\"float32\") / 255\n",
    "print(reshaped_x_train.shape, y_train.shape)\n",
    "model.fit(reshaped_x_train , y_train, epochs=1) # ,batch_size=64\n",
    "# ! why is (64,10)? (batch_size,1) is label\n",
    "model.summary()\n",
    "\n",
    "# 3 CNN 24.25% 7.4 s\n",
    "# NCN+LOSS 10.44%  39.9  # HOW TO GET 18.8 BEFORE?\n",
    "# WITHOOUT LOSS SIMILAR TIME FOR ONE layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/782 [00:01<05:49,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Step: 0 Loss: 2.3522157669067383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 102/782 [00:20<01:59,  5.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Step: 100 Loss: 2.352651834487915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 202/782 [00:41<01:55,  5.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Step: 200 Loss: 2.3527991771698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 302/782 [01:00<01:34,  5.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Step: 300 Loss: 2.352461576461792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 402/782 [01:19<01:12,  5.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Step: 400 Loss: 2.352384567260742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 502/782 [01:37<00:52,  5.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Step: 500 Loss: 2.3525073528289795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 602/782 [01:57<00:32,  5.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Step: 600 Loss: 2.3525807857513428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 702/782 [02:16<00:13,  5.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Step: 700 Loss: 2.352726697921753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [02:32<00:00,  5.14it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"4-5 Optimize and train the model in 2 style:\n",
    "    1.0 tensorflow style\n",
    "    2.0 Keras style\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"1.0 tensorflow style \"\"\"\n",
    "# 4 optimzer\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Whether `y_pred` is expected to be a logits tensor\n",
    "\n",
    "#TODO ADD hypercolumn to LCN and loss\n",
    "\n",
    "\n",
    "# 5.0 Train \n",
    "from tqdm import tqdm\n",
    "epoch_num = 1\n",
    "for epoch in range(epoch_num):\n",
    "    with tqdm(dataloader) as train_bar:\n",
    "        for step, (x,y) in enumerate(train_bar):\n",
    "            \n",
    "            with tf.GradientTape() as tape:  # in torch, without this GradientTape\n",
    "                \n",
    "                #! forward, edfault gradien zero?\n",
    "                loss = loss_fn(y, model(x)) # more with zero_grad\n",
    "                loss += sum(model.losses)# loss.backward() in torch\n",
    "                gradient = tape.gradient(loss, model.trainable_weights) # in torch training paras setting in optim\n",
    "            \n",
    "            optimizer.apply_gradients(zip(gradient, model.trainable_weights))# in torch optimizer.step()\n",
    "    \n",
    "            if step % 100 == 0:\n",
    "                print(\"Epoch\",epoch, \"Step:\", step, \"Loss:\", float(loss))\n",
    "\n",
    "\n",
    "# time = str(datetime.now()).split(\" \")[0].replace(\"-\", \"_\")\n",
    "# torch.save(model.state_dict(), \"model_{}.pth\".format(time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "functional_3 (Functional)    (64, 10)                  2188810   \n",
      "_________________________________________________________________\n",
      "softmax_1 (Softmax)          (64, 10)                  0         \n",
      "=================================================================\n",
      "Total params: 2,188,810\n",
      "Trainable params: 2,188,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 0 Step: 781 Loss: 2.552854061126709\n",
      "[0.10021344 0.09999754 0.09981877 0.09991436 0.10013708 0.09982315\n",
      " 0.10009785 0.10008855 0.10007832 0.09983091] 0\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "model.summary()  # pritn(structure)\n",
    "\n",
    "print(\"Epoch\",epoch, \"Step:\", step, \"Loss:\", float(loss))\n",
    "probability_model = tf.keras.Sequential([model, \n",
    "                                         tf.keras.layers.Softmax()])\n",
    "Predictions = probability_model.predict((x_train.reshape(50000, 32, 32, 3).astype(\"float32\") / 255)[:100])\n",
    "print(Predictions[5], np.argmax(Predictions[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Hypercolum_layer' object has no attribute '_trainable'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-0ad1f1ba09ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\"2.0 train and test in keras way \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m model = tf.keras.Sequential([model, \n\u001b[0;32m----> 3\u001b[0;31m                                          tf.keras.layers.Softmax()])\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/rds/user/hpczeji1/hpc-work/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/rds/user/hpczeji1/hpc-work/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, layers, name)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/rds/user/hpczeji1/hpc-work/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/rds/user/hpczeji1/hpc-work/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_deferred_layer_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layer_call_argspecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_inspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/rds/user/hpczeji1/hpc-work/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m_handle_deferred_layer_dependencies\u001b[0;34m(self, layers)\u001b[0m\n\u001b[1;32m    346\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_handle_deferred_layer_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;34m\"\"\"Handles layer checkpoint dependencies that are added after init.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m     \u001b[0mlayer_checkpoint_dependencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layer_checkpoint_dependencies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m     \u001b[0mlayer_to_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_checkpoint_dependencies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/rds/user/hpczeji1/hpc-work/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m_layer_checkpoint_dependencies\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m           \u001b[0;31m# Keep a separate index for layers which have weights. This allows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m           \u001b[0;31m# users to insert Layers without weights anywhere in the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/rds/user/hpczeji1/hpc-work/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mweights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \"\"\"\n\u001b[0;32m-> 1354\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnon_trainable_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/rds/user/hpczeji1/hpc-work/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mtrainable_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \"\"\"\n\u001b[1;32m   1320\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m       \u001b[0mchildren_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_children_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trainable_weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dedup_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainable_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mchildren_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/rds/user/hpczeji1/hpc-work/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_gather_children_attribute\u001b[0;34m(self, attribute)\u001b[0m\n\u001b[1;32m   2842\u001b[0m       return list(\n\u001b[1;32m   2843\u001b[0m           itertools.chain.from_iterable(\n\u001b[0;32m-> 2844\u001b[0;31m               getattr(layer, attribute) for layer in nested_layers))\n\u001b[0m\u001b[1;32m   2845\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/rds/user/hpczeji1/hpc-work/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2842\u001b[0m       return list(\n\u001b[1;32m   2843\u001b[0m           itertools.chain.from_iterable(\n\u001b[0;32m-> 2844\u001b[0;31m               getattr(layer, attribute) for layer in nested_layers))\n\u001b[0m\u001b[1;32m   2845\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/rds/user/hpczeji1/hpc-work/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mtrainable_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtrainable\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \"\"\"\n\u001b[0;32m-> 1320\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m       \u001b[0mchildren_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_children_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trainable_weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dedup_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainable_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mchildren_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/rds/user/hpczeji1/hpc-work/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mtrainable\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1246\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1247\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1248\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Hypercolum_layer' object has no attribute '_trainable'"
     ]
    }
   ],
   "source": [
    "\"\"\"2.0 train and test in keras way \"\"\"\n",
    "model = tf.keras.Sequential([model, \n",
    "                                         tf.keras.layers.Softmax()])\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=\"SGD\", loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits= True),metrics=['accuracy'])\n",
    "reshaped_train = x_train.reshape(50000, 32, 32, 3).astype(\"float32\") / 255\n",
    "print(reshaped_train.shape, y_train.shape)\n",
    "model.fit(reshaped_train , y_train, epochs=1) # ,batch_size=64\n",
    "# ! why is (64,10)? (batch_size,1) is label\n",
    "\n",
    "\n",
    "# 3 CNN 24.25% 7.4 s\n",
    "# NCN+LOSS 10.44%  39.9  # HOW TO GET 18.8 BEFORE?\n",
    "# WITHOOUT LOSS SIMILAR TIME FOR ONE layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 2.2167 - accuracy: 0.2442\n",
      "\n",
      "Test accuracy: 0.24420000612735748\n",
      "[0.08533674 0.08533674 0.08533674 0.08533674 0.08533674 0.08533674\n",
      " 0.08533674 0.08533674 0.08533674 0.23196931] 9\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "# model.summary()  # pritn(structure)\n",
    "\n",
    "# _,(x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)\n",
    "\n",
    "# print(\"Epoch\",epoch, \"Step:\", step, \"Loss:\", float(loss))\n",
    "probability_model = tf.keras.Sequential([model, \n",
    "                                         tf.keras.layers.Softmax()])\n",
    "Predictions = probability_model.predict(x_train[:100])\n",
    "print(Predictions[5], np.argmax(Predictions[5]))\n",
    "                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16c11565bb4e67660010f3b8ac54bb06ff920d6e5d1ce8d761516dd991d6b185"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('py36': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
