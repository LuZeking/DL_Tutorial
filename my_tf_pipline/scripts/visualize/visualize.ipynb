{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we visulize filters and feature maps\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from utils.dataset_loader import load_cifar10\n",
    "from backbone.cnn import CNN_3L, Sequential_CNN_3L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b53f9c5ddd8>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b54310b4208>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b53f9f5f438>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b53f9f5f668>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b53f9f56a20>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b53f9f4d5f8>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b53f9f4dda0>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b543141f0f0>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b543141fe48>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b54314271d0>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b5431427b38>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b543142f668>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b5463e294e0>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b5463e29908>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b5463e304e0>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b5463e30cc0>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b5463e39b38>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b5463e39be0>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b5463e42828>, <tensorflow.python.keras.layers.core.Flatten object at 0x2b5463e4b6a0>, <tensorflow.python.keras.layers.core.Dense object at 0x2b5463e4bf28>, <tensorflow.python.keras.layers.core.Dense object at 0x2b5463e524e0>, <tensorflow.python.keras.layers.core.Dense object at 0x2b5463e526a0>]\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "# 1&2 dataset and dataloader\n",
    "# dataloader,(x_train, y_train), (x_test, y_test)  =  load_cifar10(BATCH_SIZE = 64, buffer_size=1024)\n",
    "\n",
    "# 3 model \n",
    "model = tf.keras.applications.VGG16(\n",
    "    include_top=True,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    input_shape=None,\n",
    "    pooling=None,\n",
    "    classes=1000,\n",
    "    classifier_activation=\"softmax\",\n",
    ")\n",
    "# model.load_weight(xxx) # load pretrain, so no need for # 4&5 set optimizer and train\n",
    "model.summary()\n",
    "print(model.layers)\n",
    "print(len(model.layers))\n",
    "# [<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b8adb5ae588>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8a9da00e80>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8adb5a8ef0>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b8ae489c668>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8ae489ce48>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8ae48d49b0>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b8ae48d47b8>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8ae48df908>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8ae48e9780>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8ae48e97f0>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b8ae48ef780>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8ae48effd0>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8ae48f6dd8>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8ae4901160>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b8ae4901ac8>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8ae490a5f8>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8ae4914470>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8ae4914898>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b8ae491a470>, <tensorflow.python.keras.layers.core.Flatten object at 0x2b8ae491acf8>, <tensorflow.python.keras.layers.core.Dense object at 0x2b8ae49257b8>, <tensorflow.python.keras.layers.core.Dense object at 0x2b8ae4925a90>, <tensorflow.python.keras.layers.core.Dense object at 0x2b8ae4925fd0>]\n",
    "# len = 23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot activation/feature maps\n",
    " - visualize model.layer.output with specific images\n",
    "   - layer_outputs --> activation_model --> activation_model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x2ba23bafe2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "first_layer_activation size : (1, 14, 14, 512)\n",
      "Img 0 : mean energy value in layer 4: (49.725147, 466.10052, 0.0, 2509.7964)\n",
      "sumed first_layer_activation size : (1, 14, 14)\n",
      "mean/max: (0.31288138, 49.725136, 158.92648)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# instantiating a model from a input tensor and all imtermidate out put tensor \n",
    "layer_outputs = [layer.output for layer in model.layers[:]]\n",
    "activation_model = keras.models.Model(inputs = model.input, outputs = layer_outputs)\n",
    "\n",
    "img_tensor = np.random.rand(224,224,3) * 255 # tf.tensor(np.random((224,224,3)))\n",
    "from PIL import Image\n",
    "img_tensor = Image.open(\"/home/hpczeji1/hpc-work/Codebase/Datasets/healthy_aging/marleen_dataset/faces/17.jpg\")\n",
    "img_tensor = img_tensor.resize((224,224))\n",
    "# from PIL import Image\n",
    "# img_tensor = Image.fromarray(img_tensor.astype('uint8')).convert('RGBA')\n",
    "batch_size = 1\n",
    "imgs_batch_tensor = np.empty((batch_size,224,224,3))\n",
    "imgs_batch_tensor[0,:,:,:] = img_tensor \n",
    "\n",
    "activations = activation_model.predict(imgs_batch_tensor)\n",
    "first_layer_activation = activations[14] # 第四个 print(first_layer_activation.shape) 第三个是-13\n",
    "import matplotlib.pyplot as plt\n",
    "print(f\"first_layer_activation size : {first_layer_activation.shape}\") # batch_num, w,h,c\n",
    "#! why activation value is so huge than 256\n",
    "print(f\"Img 0 : mean energy value in layer 4: {np.mean(first_layer_activation[0, :, :, :]), np.max(first_layer_activation[0, :, :, 1]),np.min(first_layer_activation[0, :, :, :]),np.max(first_layer_activation[0, :, :, :])}\")\n",
    "plt.matshow(first_layer_activation[0, :, :, 4], cmap='viridis') # channel 4\n",
    "\n",
    "# first_layer_activation = tf.reduce_max(first_layer_activation,axis=3)\n",
    "# print(f\"sumed first_layer_activation size : {first_layer_activation.shape}\") \n",
    "# plt.matshow(first_layer_activation[0, :, :], cmap='viridis')\n",
    "# energy_map = first_layer_activation[0, :, :]\n",
    "# print(f\"mean/max: {np.mean(energy_map)/np.max(energy_map), np.mean(energy_map),np.max(energy_map)}\")\n",
    "\n",
    "# energy map\n",
    "first_layer_activation = np.mean(first_layer_activation,axis=3)  # np.max\n",
    "print(f\"sumed first_layer_activation size : {first_layer_activation.shape}\") \n",
    "plt.matshow(first_layer_activation[0, :, :], cmap='viridis')\n",
    "energy_map = first_layer_activation[0, :, :]\n",
    "print(f\"mean/max: {np.mean(energy_map)/np.max(energy_map), np.mean(energy_map),np.max(energy_map)}\")\n",
    "\n",
    "plt.savefig(\"results/sumed_first_layer_activation.jpg\")\n",
    "\n",
    "#! 1 above plot activations, and 2 follow later link can plot filters(weight units) \n",
    "# https://machinelearningmastery.com/how-to-visualize-filters-and-feature-maps-in-convolutional-neural-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot filters/weights\n",
    " - filters,biases = model.layer.get_weights(),then visualize the filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block1_conv1 (3, 3, 3, 64)\n",
      "block1_conv2 (3, 3, 64, 64)\n",
      "block2_conv1 (3, 3, 64, 128)\n",
      "block2_conv2 (3, 3, 128, 128)\n",
      "block3_conv1 (3, 3, 128, 256)\n",
      "block3_conv2 (3, 3, 256, 256)\n",
      "block3_conv3 (3, 3, 256, 256)\n",
      "block4_conv1 (3, 3, 256, 512)\n",
      "block4_conv2 (3, 3, 512, 512)\n",
      "block4_conv3 (3, 3, 512, 512)\n",
      "block5_conv1 (3, 3, 512, 512)\n",
      "block5_conv2 (3, 3, 512, 512)\n",
      "block5_conv3 (3, 3, 512, 512)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 18 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\"\"\"Visualize layer weights, refer to  https://machinelearningmastery.com/how-to-visualize-filters-and-feature-maps-in-convolutional-neural-networks/\n",
    "\"\"\"\n",
    "# see how many onv layer we have\n",
    "for layer in model.layers:\n",
    "    if \"conv\" not in layer.name:\n",
    "        continue\n",
    "    # get filter\n",
    "    filters, biases = layer.get_weights()\n",
    "    print(layer.name,filters.shape)\n",
    "\n",
    "filters,biases = model.layers[1].get_weights()\n",
    "f_min,f_max = filters.min(),filters.max()\n",
    "filters = (filters-f_min)/f_max-f_min\n",
    "# plot some filters in layer 0,which only has 3 channels\n",
    "\n",
    "n_filters,ix=6,1\n",
    "for i in range(n_filters):\n",
    "    f = filters[:,:,:,i]\n",
    "    for j in range(3):\n",
    "        ax = plt.subplot(n_filters,3,ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\t\t# plot filter channel in grayscale\n",
    "        plt.imshow(f[:, :, j]) # , cmap='gray'\n",
    "        ix += 1\n",
    "\n",
    "plt.show()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  calculate complexity in dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating a model from a input tensor and all imtermidate out put tensor \n",
    "layer_outputs = [layer.output for layer in model.layers[:]]\n",
    "activation_model = keras.models.Model(inputs = model.input, outputs = layer_outputs)\n",
    "\n",
    "# img_tensor = np.random.rand(224,224,3) * 255 # tf.tensor(np.random((224,224,3)))\n",
    "# from PIL import Image\n",
    "# img_tensor = Image.open(\"/home/hpczeji1/hpc-work/Codebase/Datasets/healthy_aging/marleen_dataset/faces/17.jpg\")\n",
    "# img_tensor = img_tensor.resize((224,224))\n",
    "# from PIL import Image\n",
    "# img_tensor = Image.fromarray(img_tensor.astype('uint8')).convert('RGBA')\n",
    "cat_dict = {\"faces\":1000,\"animals\":2000,\"places\":3000,\"objects\":4000}\n",
    "complexity_list = []\n",
    "for cat_name in cat_dict.keys():\n",
    "    print(f\"Caculating {cat_name} ...\")\n",
    "    source_dir = f\"/home/hpczeji1/hpc-work/Codebase/Datasets/healthy_aging/marleen_dataset/{cat_name}/\"\n",
    "    batch_size = 1000\n",
    "    imgs_batch_tensor = np.empty((batch_size,224,224,3))    \n",
    "    for i in range(batch_size):\n",
    "        img_tensor = Image.open(source_dir+f\"{i+cat_dict[cat_name]-1000}.jpg\")\n",
    "        img_tensor = img_tensor.convert('RGB')\n",
    "        # img_tensor = img_tensor.resize((224,224))\n",
    "        # print(f\"shape:{img_tensor.size}\")\n",
    "        try:\n",
    "            imgs_batch_tensor[i,:,:,:] = img_tensor \n",
    "        except:\n",
    "            print(f\"img_tensor.size:{img_tensor.size}\")\n",
    "            raise NotImplementedError\n",
    "\n",
    "    activations = activation_model.predict(imgs_batch_tensor)\n",
    "    forth_layer_activation = activations[14] # 第四个 print(first_layer_activation.shape) 第三个是-13\n",
    "    complexity_list += [np.mean(forth_layer_activation[i, :, :, :]) for i in range(1000)] \n",
    "\n",
    "# write to csv\n",
    "with open(\"/home/hpczeji1/hpc-work/Codebase/Datasets/healthy_aging/marleen_plot/Id_Complexity.csv\", \"w\") as f:\n",
    "    f.write(\"{Id},{Complexity}\\n\")\n",
    "    for i in range(4000):\n",
    "        f.write(\"{},{}\\n\".format(i, round(complexity_list[i]),0))\n",
    "\n",
    "# https://machinelearningmastery.com/how-to-visualize-filters-and-feature-maps-in-convolutional-neural-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot hist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cat_dict = {\"faces\":1000,\"animals\":2000,\"places\":3000,\"objects\":4000}\n",
    "cat_name = \"faces\"\n",
    "for cat_name in cat_dict.keys():\n",
    "    n, bins, patches = plt.hist(x = complexity_list[cat_dict[cat_name]-1000:cat_dict[cat_name]], bins =list(range(1,101,3)),color='#0504aa',alpha=0.5, rwidth=0.85)  # bins = [round(0.001*i,2) for i in list(range(901,1021,10))]\n",
    "\n",
    "    # print([0.01*i for i in list(range(0,100,2))])\n",
    "    plt.ylim(0,150)\n",
    "    plt.xlim(0,100)\n",
    "\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'{cat_name.capitalize()} Complexity Distribution')\n",
    "\n",
    "    save_dir = \"/home/hpczeji1/hpc-work/Codebase/Datasets/healthy_aging/marleen_plot/categories/\"\n",
    "    plt.savefig(save_dir + f'{cat_name.capitalize()}_Complexity_Distribution.jpg', bbox_inches = 'tight')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.1745567\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16c11565bb4e67660010f3b8ac54bb06ff920d6e5d1ce8d761516dd991d6b185"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('py36': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
