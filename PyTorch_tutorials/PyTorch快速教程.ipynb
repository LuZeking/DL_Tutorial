{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# PyTorch快速教程\n",
    "\n",
    "## 简介 [参考60分钟学会PyTorch]\n",
    "\n",
    "本教程将帮你掌握Keras和TensorFlow的基本用法和核心概念，其中包括：\n",
    "\n",
    "- 张量（tensor），变量（variable），梯度（gradient）。\n",
    "- 通过继承`Layer`类来建立自定义的神经网络层。\n",
    "- 手写循环来更好地自定义对神经网络训练过程。\n",
    "- 使用`add_loss()`方法来自定义神经网络层的损失函数。\n",
    "- 在手写循环训练过程中对评估标准（metrics）进行追踪。\n",
    "- 使用`tf.function`来编译并加速神经网络的执行过程。\n",
    "- 神经网络层的运行：训练模式 vs 推断（inference）模式。\n",
    "- Keras的函数式API。\n",
    "\n"
   ],
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1 张量操作、运算、梯度\n",
    "### 1.1 张量操作、变量和计算\n",
    "总之，深度学习存储和操作数据的主要接口是张量（ n 维数组）。它提供了各种功能，包括基本数学运算、广播、索引、切片、内存节省和转换其他Python对象。\n",
    " - 可类比numpy运算（cat,广播机制，切片索引）\n",
    " - 可以使用切片表示法将操作的结果分配给先前分配的数组，or 可以使用X[:] = X + Y或X += Y来减少操作的内存开销\n",
    " - 转换为NumPy张量很容易，反之也很容易。转换后的结果不共享内存。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\r\n",
    "\r\n",
    "x =None\r\n",
    "y = None\r\n",
    "\r\n",
    "torch.cat((x,y),dim = 0) # concat\r\n",
    "torch.cat((x,y),dim = 1)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got NoneType",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d593c5999cf6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# concat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got NoneType"
     ]
    }
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 数据预处理\n",
    "总的而言，pandas可以和pytorch无缝衔接\n",
    "- open写入 & pandas读取\n",
    "- 处理缺失值\n",
    "- pd读取的numpy格式，可以直接通过tensor转化为张量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import tensorflow as tf\r\n",
    "print(tf.__version__)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3 线代计算\n",
    "本章含练习：李沐2.3节\n",
    "- 什么是标量，变量，向量，矩阵？\n",
    "- 如何实现矩阵的运算、降维等\n",
    "- 如何实现矩阵向量点积，矩阵乘法有何关系？\n",
    "- 什么是范数,如何实现？"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### * 概率学&查阅文档很重要，待看\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4 微分自动求导/梯度计算\n",
    "含练习\n",
    "深度学习框架可以自动计算导数。为了使用它，我们首先将梯度附加到想要对其计算偏导数的变量上。然后我们记录目标值的计算，执行它的反向传播函数，并访问得到的梯度。\n",
    "- 微分参考数学的本质理解\n",
    "- PyTorth 只需设置backward反向传播后自动.grad求出梯度\n",
    "- 当y不是标量时，向量y关于向量x的导数的最自然解释是一个矩阵。\n",
    "- 可以分离进行求导计算\n",
    "- 使用自动求导的一个好处是，即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "x = torch.arange(4.0)\n",
    "x.requires_grad_(True)\n",
    "x.grad\n",
    "y = 2*torch.dot(x,x)\n",
    "y\n",
    "\n",
    "y.backward()\n",
    "x.grad"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2 通过继承`Layer`类来建立自定义的神经网络层\n",
    "### 2.1 线性神经网络为例\n",
    "### 2.2 MLP\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "interpreter": {
   "hash": "c5e48f84046969b800ff52f6d80523bcd1ca3fb1a99f1449e4197bf6c73dc096"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}