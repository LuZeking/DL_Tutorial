{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "How to finetune from Cifar10 to Minst\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"I Target： Here we learn how to train model\n",
    "\n",
    "II Definition: \n",
    "    1 dataset definition and dataloader\n",
    "    2 model\n",
    "    3 optimizer and eval\n",
    "    \n",
    "III Instances:\n",
    "    # 1 following sample classifier\n",
    "    #TODO 2 finetune --> change state_dict?\n",
    "    #! 3 add sptial loss from BasNet\n",
    "    # 4 eval&analyse part\n",
    "    #TODO * same in Keras/tensorflow\n",
    "\n",
    "\n",
    "IV Compare 2 then Generalize\n",
    "\n",
    "V Test in New instance \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0014, 0.0000, 0.0492,  ..., 0.0244, 0.0000, 0.0000],\n",
      "        [0.0122, 0.0000, 0.0530,  ..., 0.0216, 0.0000, 0.0000],\n",
      "        [0.0168, 0.0000, 0.0494,  ..., 0.0188, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0288, 0.0000, 0.0671,  ..., 0.0047, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0378,  ..., 0.0189, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0425,  ..., 0.0082, 0.0000, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 1 dataset\n",
    "\"\"\"Inherit from Dataset class or load inner torchvision.dataset\"\"\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5), std = (0.5))  # normalize to [-1,1]\n",
    "])\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "train_dataset = MNIST(root=\"/home/hpczeji1/hpc-work/Codebase/Datasets/mnist_data\",\n",
    "                      train=True,\n",
    "                      transform=transform,\n",
    "                      target_transform=None,  # Eg1.2.1 : <class 'int'>\n",
    "                      download=False)\n",
    "\n",
    "# 2  dataloader\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=10000,\n",
    "                          shuffle=True)\n",
    "\n",
    "\n",
    "# 3 model\n",
    "from torch import nn\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,out_channels=64,kernel_size=(3,3),stride=3) # in_channel is te image channel number\n",
    "        self.conv2 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size=(3,3),stride =3)\n",
    "        self.conv3 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size=(3,3),stride =3)\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        self.Flatten = nn.Flatten(start_dim=1, end_dim=-1) # (B,C,H,W), USE C,H,W\n",
    "        self.Linear = nn.Linear(in_features=64*1*1, out_features=10,bias= False) # ins_feature num = flatten\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        # print(\"[before flatten] x.shape: {}\".format(x.shape))  # torch.Size\n",
    "        x = self.Flatten(x)\n",
    "        # print(\"[after flatten] x.shape: {}\".format(x.shape))  # torch.Size([1, 3920])\n",
    "        x = self.Linear(x)\n",
    "\n",
    "        return self.relu(x)\n",
    "\n",
    "model = SimpleModel()\n",
    "\"\"\"if have pth, can load this pth weight and use model(x) to interference\"\"\"\n",
    "# model.load_state_dict(torch.load(\"./model_2021_11_28.pth\")) \n",
    "# print(model(x))\n",
    "\n",
    "# 4 optimizer\n",
    "optimizer = optim.SGD(params=model.parameters(), lr=0.001, momentum=0.9)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 5 train\n",
    "epoch_num = 2\n",
    "for epoch in range(epoch_num):\n",
    "    with tqdm(train_loader) as train_bar:\n",
    "        for x,y in train_bar:\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(model(x),y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    print(f\"epoch:{epoch}, loss{loss}\")\n",
    "\n",
    "time = str(datetime.now()).split(\" \")[0].replace(\"-\", \"_\")\n",
    "torch.save(model.state_dict(), \"model_{}.pth\".format(time))\n",
    "\n",
    "print(\"~~~~~~撒花~~~~~~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16c11565bb4e67660010f3b8ac54bb06ff920d6e5d1ce8d761516dd991d6b185"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('py36': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
